                           III Octree Structure
3.1 Describing the Octree Structure
   The octree structure is an abstract data type in which an area in
3-space is evenly divided into 8 subdomains. In its current
implementation, the octree used begins with a root node that can be
thought of as representing a bounding box of the total area being
examined [4] [6] [7] [9]. Therefore, there should not exist objects
having coordinates greater than the maximum bounds, nor objects having
coordinates less than the minimum bounds of the root node. There
should be only one root node representing the total area, called the
"global root," and knowledge of this root should be given to all the
processors if the octree is a distributed octree.

   The minimum and maximum bounds of the root octant give the minimum
and maximum coordinates bounds of the space the objects lie in. This
bounding box need not be a perfect square, but each face of the box
needs to be a regular rectangle, i.e. each corner has to be at 90
degree angles. A bounding box, whether it is for the global view or a
subdomain, is represented by exactly one node in the octree
structure. When subdivided, corresponding to one level of refinement,
this node will have exactly eight child nodes associated to it in the
tree structure. This subdivision represents dividing the bounding box
using three cutting planes, each parallel to a major axis plane, and
each dividing the box evenly in half. This is the reason for having
the corners being at 90 degrees; if the angles were not, then the
cutting planes used would not evenly divide the region. (It may be
useful as future work to incorporate more flexibility into the octree
structure.) Each of the smaller bounding boxes created is represented
by a child node in the octree structure. Minimum and maximum
coordinate bounds of the smaller box are stored within the new child
node. The order in which the children are assigned subdomains
corresponds to the space filling curve specified by the user.

   A child node need not be on the same processor as its parent
node. If child nodes exits on a separate processor, then the octree is
said to be distributed [7] [9]. In this manner, subdomains being
worked on are partitioned among different processors. Information
gathered on each subdomain assigned to an octant, combined with octree
ordering and traversal algorithms, can be used to balance work loads
among these processors.

3.2 Octant Data Structures
   Listed below is a description of some of the data structures used
by the octree load balancer/partitioner. It describes the data stored
within each structure and what it is used for.

3.2.1 Data Stored Inside an Octant
   Contained within a node of the octree, known as an octant, are data
used by the program to help maintain the tree structure. When memory
for the octant is first allocated, most of the data fields are
initialize with 0 or -1, and all pointers are set to NULL. This
indicates that there are no associations with this octant, and
therefore it is a new octant. The first few fields that are set are
the ppid and the octant id. The ppid is the processor id of the parent
the octant is associated with. A ppid of -1 means that it has no
parent, and is used to indicate a possible global root. The octant id
is a locally unique number identifying the octant. This is useful for
ordered traversal and searching for a particular octant. If the
parent is on the same processor as the new node, then the "parent"
field, which is a pointer to an octant, is set to point to the parent
octant, otherwise it stays NULL.

   The next set of data fields deals with the tree structure. The
"which" field indicates the octant's child numbering, telling which
child of the parent is this octant. The valid numbers are 0 - 7 if it
is a child of some parent, and a -1 if it is a global root. The
"numChild" field gives the number of child nodes that the octant points
to. If the number is 0, then the octant is a terminal octant and is a
leaf node in the tree structure. If "numChild" is greater than 0, then
the octant has children associated with it. These children are pointed
to by "child[]," which is an array of 8 pointers to octants. The
"cpid[]" array is a list of all the processor ids that the children
are on. This is useful for telling if a child is local or off
processor. These structures do allow for flexibility in the number of
children an octant can have, but the current software that uses this
octree structure usually looks to see if there are exactly 8 children,
or 4 if the algorithm is scaled back to 2 dimensions and uses a
quadtree structure instead.

   Octant ownership of objects is represented the next set of fields
in the octant structure. The min and max fields give the bounds of the
area in space that belongs to a particular octant. This gives
information on the bounding boxes which will indicate whether or not
an object should be associated with the octant. Objects that lie
exactly on the bounds have to be dealt with as a special case. The
object will be given to the lowered numbered octant, with the octant
ordering determined by on the space filling curve used. Once an object
is assigned to an octant, it is placed on a linked list of objects
pointed to by "list," which is a pointer to a structure "Region." The
maximum number of objects that can be associated with an octant is a
user-specified input parameter used in creating the octree. The only
real bounds to the number of objects are from memory constraints.

   The final sets of data deal with load balancing information. The
cost of an octant is stored inside "cost," which is the total of all
the costs of an octant's children. If the octant is a terminal octant,
then its costs is the summed weight of all the objects associated with
that octant. The cost is used to determine the load on a processor,
and to determine if an octant should lie within the current
partition. The partition number gives the information on which
processor to migrate objects, and is stored in the data field "npid."
This is set during the load balancing phase, and its default value is
the local processor id.

3.2.2 Region Structure
   Stored within a terminal octant is a list of objects associated
with that octant. In the load balancer code, these are the objects
that define the load on a processor and are given by the structure
LB_TAG. Along with the LB_TAGs, the weight and locality (coordinates
of an object in 2-space or 3-space) are stored inside the Region
structure. (The name "Region" is carried over from RPI's development of
the octree algorithm, but efforts have been made to make a more general
implementation. The name "Region" should not give any connotation of
the types of objects being balanced.) A pointer to a Region is also
included so that a dynamically allocated list of objects can be
associated with an octant. Finally, the "attached" field is to indicate
whether the object should be considered orphaned or not. An orphaned
object is an object that cannot be placed on its current processor and
needs to be migrated to a processor that owns the subdomain that the
object lies in.

3.3 Space Filling Curves
   Space filling curves (SFC) are used to linearly order points of a
multidimensional array [1] [2] [5]. The ordering should maintain the
proximity of the points, so that points that are close to each other
in the multidimensional array should be relatively close to each other
in the linear ordering. Similarly, points far from each other should
be relatively far from each other in the linear ordering. If entries
in the multidimensional array were points on a mesh, then given enough
refinement of the mesh, the traversal of the points in the linear
ordering would create a curve that would fill that mesh to any
specified tolerance, thus the name space filling curve. Space filling
curves are recursive in design so that they can be expanded to any
size and theoretically in to any dimension. The space filling curve
methods used in the Octree Partitioning software are described below.

3.3.1 Morton Indexing (Z-Curve)
   The first curve implemented in the octree structure was programmed
at Rensselaer Polytechnic Institute [4] [6] [7] [9]. This method,
called Morton Indexing, also known as z-curve, creates a "Z" shaped
pattern that gives the SFC its more common name. The pattern can be
considered as follows (in 2D, but it is expanded into 3D as well):
Given a square mesh region in 2D, subdivide the region into 4 equal
subregions. For ease of reference, let the quadrants be numbered in
the usual mathematical quadrant convention, where the upper right is
quadrant I, upper left quadrant II, lower left quadrant III, and
finally the lower right quadrant IV. The z-curve follows the pattern
of quadrants III, IV, II, I, thus creating a curve similar to a
reflected "Z". This is very similar to a binary numbering of the
quadrants in which the order 00, 01, 10, 11 represent YX pairs of
coordinates. For a larger mesh of the size 2^k X 2^k, the recursive
pattern is to have four smaller meshes of size 2^(k-1) X 2^(k-1) each
traversed in the same "Z" pattern. Each smaller mesh would then have
smaller meshes traversed in the same pattern, until the bottoming out
condition of a 2 X 2 mesh, which was described above. Expanding into
3-dimensions, instead of dividing a domain into four quadrants, a
space is divided into eight octants, four octants on one level, and
four more above. The 3D expansion of the curve is to traverse the
lower four in the z-curve order, then go up to the second level and
repeat the pattern. This is again extending the bit patterns into 3D
with 000, 001, 010, 011, 100, 101, 110, 111, where the bits represent
ZYX coordinates sets.

3.3.2 Gray Code
   This addition uses the Gray encoding scheme on the bits used to
represent coordinates sets. Gray code is a method of encoding a series
of numbers so that between any two sequential numbers there is only
one bit difference [1] [2] [11]. Using this on the bit encoding for
the z-curve, in 2D the new pattern would become 00, 01, 11, 10,
representing a quadrant traversal III, IV, I, and II. Using the
z-curve method of recursion, each sub-quadrant below one of these
initial quadrants would follow the same pattern. Examining this
pattern, this creates a curve that is self-intersecting, which causes
this to not be classified as a space filling curve. It is a basis for
the actual Gray Curve. Due to time constraints this method was left as
it is. If interested, this can be pursed as a future work. To correct
the problem, a flip on an axis of rotation is needed on two of the
quadrants. This will remove the self-intersecting pattern. It is
interesting to note, that if Gray Curve SFC was implemented, then
there would exist more "jumps" in the traversal of points in the
multidimensional array. These jumps represent areas where
discontinuous partitions may arise. The Gray code can also be applied
in 3D giving the pattern 000, 001, 011, 010, 110, 111, 101, 100.

3.3.3 Hilbert Curve
   The final SFC implemented was the Hilbert Curve. The major
advantage of the Hilbert curve is that it avoids any sudden jumps that
exist in the z-curve and Gray code [1] [2]. This helps to prevent any
discontinuous partitions that may occur during the partition phase of
the load balancer. The encoding code segment for the Hilbert curve was
written by H. Carter Edwards [5] of the SIERRA project at Sandia
National Laboratories. Given an array of normalized points as input,
the code returns those points in the order they would appear in a
Hilbert curve traversal. Since the Hilbert curve is an extension of
the Gray code [10], it starts with a similar pattern, but with each
level of grid refinement, sections of the curve are rotated and/or
flipped to maintain continuity in the curve. See [1] [2] [5] [10] for
more detail.


                           IV. Octree Functions [7] [9]

4.1 POC_init()
   Initialize all the global variables used on a processor. These
global variables include OCT_count, idcount, and dimension. OCT_count
gives the total number of octants created on the processor. idcount
is a count of the ids used locally. The idcount helps insure locally
unique numbering of the octants. dimension tells the code whether the
algorithms used should be in 2D or 3D. 

4.2 POC_malloc();
   Allocates space in memory for a new octant. Nothing is
initialized. A pointer to the new octant is returned.

4.3 POC_new()
   Creates a new octant, by calling POC_malloc(), on the local
processor and return a pointer to it. It will have no parents or
children. It will be initialized with a locally unique identification
number. Identification numbers are assigned sequentially starting from
0 and incrementing by 1 each time a new octant is made.

4.4 POC_free()
   Frees space in memory used by the octant passed into this
function. POC_free() does not delete the attached objects associated
with the octant; before calling, the user must specifically call
POC_clearRegions(). Memory leaks will occur otherwise.

4.5 POC_id()
   Returns the identification number of an octant specified.

4.6 POC_setparent()
   Sets the parent pointer of an octant to point to its parent. Also
sets the parent processor id number. If the parent processor id number
is not equal to the octant's processor id number, then the octant is
added to the local root list.

4.7 POC_setchildnum()
   Sets the child array index number of the octant.

4.8 POC_childnum()
   Returns the child array index number of the octant.

4.9 POC_setchild()
   Given a parent octant, a child octant, and an integer i, this will
set the ith child pointer of the parent octant to point to a child
octant.

4.10 POC_setchildren()
   Given an array of children and an octant, this will set all the
child pointers of the octant to point to each child in the array. The
children are attached to the octant in the order that they are placed
in the array.

4.11 POC_setbounds()
   Sets the minimum and maximum domain bounds associated with an octant.

4.12 POC_bounds()
   Retrieves the minimum and maximum domain bounds associated with an
octant. The bounds are placed inside the two arrays of doubles passed
to the function.

4.13 POC_parent()
   Returns a pointer to the parent of the octant.

4.14 POC_child()
   Given an octant and an integer i, this returns a pointer to the
ith child of the octant.

4.15 POC_children()
   Given an array of pointers to octants, this function will fill in
values of all an octant's children .

4.16 POC_isTerminal()
   Returns TRUE (1) if the octant is a terminal octant. A terminal
octant is an octant that has no children associated with it.

4.17 POC_regionlist()
   Returns a pointer to the linked list of objects associated with an
octant. This can be used to iterate through the list and find
information on the objects associated with an octant.

4.18 POC_addRegion()
   Adds an object to the linked list of objects associated with an octant.

4.19 POC_remRegion()
   Removes a specific object from the linked list of objects, and
frees any memory used.

4.20 POC_clearRegions()
   Removes all the objects associated with an octant and frees space
used by the list.

4.21 POC_nRegions()
   Returns the number of objects associated with an octant.

4.22 POC_localroots()
   Returns a pointer to the linked list of subtree roots that are
local to the processor.

4.23 POC_modify_cost()
   Assign the cost field of the octant data structure.

4.24 POC_modify_newpid()
   Assign the npid field of the octant data structure. This tells
where the octant should be migrated to.

4.25 POC_data_cost()
   Returns the cost associated with an octant.

4.26 POC_data_newpid()
   Returns the npid associated with an octant.

4.27 POC_nlocal()
   Returns the number of local leaf nodes in a subtree of the root
passed in.

4.28 POC_nOctants()
   Returns the number of octants created on the local processor.

4.29 POC_printResults()
   Prints the intermediate results of what has been done to
the octree. It uses the recursive call POC_DfsTraversal() to iterate
through a subtree in a depth first search pattern. This was used for
debugging purposes to see what was happening with the tree.

4.30 POC_DfsTraversal()
   Recursively goes down a subtree in a depth first search pattern. If
a leaf node is visited, the POC_printRegionInfo() is called to get
object information.

4.31 POC_printRegionInfo()
   Various print statements used to track what is happening with the
data in the tree. Most print statements print to standard error so
that usual output can be ignored through a output redirection. Mainly
used for debugging.

4.32 POC_nextDfs()
   Returns a pointer to the next octant that should be visited in a
depth first search order.

4.33 POC_local()
   Given an octant and an integer i, returns TRUE (1) if the ith
child of the octant is local to the processor.

4.34 POC_setCpid()
   Given an octant, the child's processor id, and an integer i, this
sets the ith entry in the octant's cpid array to the child's processor
id number.

4.35 POC_delTree()
   Recursive call that traverses down root's subtree deleting all the
octants in that subtree. All objects associated in that subtree are
also deleted. All memory used by the tree are freed.


                         V. Load Balancing Functions

5.1 Parameters and Defaults
   An array of parameters that can be retrieved informs the load
balancing routine of user specifications for the routines
used. Parameter 0 specifies the number of dimensions in the
application's domain. The valid argument is either 2 or 3, with the
default being 3 since the 3D algorithm can handle a greater variety of
problems. The 3D algorithm can be used for 2-dimensional meshes, but
there is a lot of wasted memory as the octants that have z-coordinate
minimum bounds greater than 0 are stored but not used. Conversely, the
2-dimensional algorithm can be used for 3D surface curves provided
that the surface can be projected onto the xy-plane without
overlapping points. It may be useful for future work to find other
projections to use with 3D surface curves, as a way of making the code
more efficient.
   Parameter 1 specifies which space filling curve algorithm to
use. The default, input of 0, is the Morton Indexing scheme that uses the
z-curve. This was the original method used at Rensselaer Polytechnic
Institute, and is the fastest of the three choices. Gray code, input
of 1, uses the Gray encoding method to order the octants. There are
less jumps, but it is slightly slower. The last choice, input of 2, is
the Hilbert curve. This is the most expensive of the three routines
implemented, but it also guarantees continuous domain decomposition
during the partitioning phase of the load balancer. The user has to
decide which method best suits the needs of the application. See
section 3.3 for more information on space filling curves.
   Parameter 2 is used to tell the maximum number of objects to be
associated with an octant. If parameter 2 equals 1, each object is
placed individually into a partition. This is a slower method, but it
gives the best balance between processors. With higher numbers, groups
of objects are placed in a partition as one set. If a group does not
fit perfectly in a partition, a decision is made by the partitioner
whether to assign it to one partition or another. This decision is
made based on the cost of the whole group. For large numbers of
objects being balanced, it is recommended that a higher number be
used, but the default is set to 1 to try and give the best balancing.
   The last parameter deals with statistics on the load balancer. An
input of 0 means the user does not wish to see the statistics output,
and no information on the load balancer is given. The default, input
of 1, gives a general overview of what happened during load
balancing. The Total time, in seconds, used by the load balancer is
given along with averages on the partitioner iterations, number of
objects sent and received during different parts of the code, the load
averages, and the times for startup, balancing, and
communication. This gives the user a general idea of what happened. An
input of 2 causes information to be printed out on each of the
processors used. Global averages and averages per processor are
printed to standard output.

5.2 Generating the Initial Octree Structure
   The octree partitioner and load balancer depend on the octree
structure when computing the partitions; thus an octree must be built
for all the algorithms used. The initial tree is built in the function
oct_gen_tree_from_input_data(). When this function is called, an LB
structure is passed in from the application calling the octree load
balancer. This structure allows the program to retrieve object
information the processors use to determine work loads and, in turn,
determine the partitioning for load balancing. The get_bounds()
function is called after receiving the LB structure to get the
bounding box information needed to create the octree structure.

   The function get_bounds() is used to find the global minimum and
maximum coordinates bounds of all objects that are to be
balanced. get_bounds() first retrieves the total number of local
objects on an individual processor. Using this information, it then
iterates through the list of objects storing all important information
inside an object list, as well as calculating the min and max
coordinates of all local objects. The object list is a linked list of
Region structures that hold information on the objects, such as a
LB_Tag, coordinate location, and weight. Nodes of the list will be
placed into octants once the initial octree has been built. The global
min and max are found using the MPI function MPI_Allreduce(). The
global min and max are needed for the octree structure because it
needs to create an octant that will encompasses all the data. From
this first octant, the program will subdivide as needed creating the
octree structure.

   Before building the tree, the program also needs to know how many
levels of refinement are needed so that there is an adequate number of
leaf nodes to divide among the processors. When an octant is refined,
the bounds of the octant are evenly divided and stored among eight
child octants. Starting with the global root, the number of leaf nodes
is 1. If there is more than one processor, the global root should
subdivide into 8 leaf nodes. If there are greater than 8 processors,
then each of the leaf nodes should subdivide again giving a total of
64 nodes. This is done so that the level of refinement is even when
distributing among the processors. The number of leaf nodes created is
8^(level of refinement). The level of refinement is calculated based
on the number of processors used. Level of refinement = ceiling(log
base 8(number of processors)).

   With the global min and max values and the level of refinement
known, all processors begin building a tree. Each processor creates a
root using the global min and max as its bounds. The root is then
subdivided to the level of refinement mentioned above. The subdomains
are assigned to the leaf nodes in the order of the space filling curve
specified by the user. The leaf nodes are then divided among the
processors in a depth first search ordering, so that processor 0 will
get the first few leaves, processor 1 the next few, and so forth. In
the case where the number of processors does not evenly divide the
number of leaves, the leaves are distributed as evenly as possible
with extra leaves given to the higher numbered processors. This helps
offset the fact that if an object should lie exactly on the boundary
of two octants, the default is that the object belongs to the lowered
numbered octant. Subdomains were calculated on each processor so that
knowledge of subdomain ownership would be known globally. Each
processor then begins fitting, based on coordinate information, the
object from their object list into their subdomain. Any objects whose
coordinates are not within any local roots' subdomains are then
considered to be orphans. Orphans are then migrated to the appropriate
processor using the information calculated when creating the tree and
the function migreg_migrate_orphans().

   All orphaned objects are passed into the function
migreg_migrate_orphans(). Each orphan's coordinates are examined and
the processor which the orphan should be migrated to is determined
using a logical octree search algorithm, O(log base 8(n)), and the
initial tree distribution created previously. Messages to be sent off
processor containing the orphaned objects are packaged using the
irregular communication functions, comm_create() and comm_do(),
develop by Steve Plimpton and Bruce Hendrickson of Sandia National
Laboratories. Once the orphans are on the correct processor, they are
inserted into the octree structure, thus creating the distributed
octree.

   The setup for the initial tree can be a very costly portion of the
octree partitioning/load balancing program. The expense can greatly
increase depending on the ordering used for the octree. The initial
tree is built in this manner to create a grouping that when balanced,
with the dfs_partition() algorithm, a better partitioning can be
created. This is especially true if the initial mesh partition is not
close to an octree partitioning. Still individual results will vary,
since this does not guarantee contiguous partition regions. The faster
ordering system, the Morton indexing scheme, has a greater possibility
for discontinuous subdomain regions after load balancing, than
compared to the more expensive ordering system, the Hilbert Space
Filling Curve. A trade-off occurs between quality of partitions versus
cost. It is up to the user to decide what is more important, thus
deciding which ordering system to use.

5.3 Inserting Object Information Into The Tree
   Inserting objects into the octree begins with a list of objects
being passed into the function oct_global_insert_object(). This
function iterates through the list, and tries to find an octant in
which to insert each entry. It retrieves the next object on the list,
then examines all the local roots to see which, if any, local root has
the subdomain where the object belongs.  A list of local roots is
obtained with the POC_localroots() function which returns a pointer to
a linked list of all the roots local to the processor. The root list
is traversed, and for each root, the min and max bounds are compared
to the coordinates of the object to be inserted. If the coordinates do
not lie within the bounds, then the next root is tested. If no root is
found, the object is marked as an orphan which will be migrated to
another processor that owns the subdomain containing the object. If a
root was found, then the exact octant the object is to be inserted in
must be found. A recursive function, oct_findOctant(), is called with
the object and the local root passed in as an argument. It traverses
the tree by starting at the root passed in and determining which child
has the correct min and max bounds. oct_findOctant() then recursively
calls itself with the child octant as the new root. The bottoming out
condition is when the root octant passed in is a terminal octant, a
leaf node in the octree structure. If a local octant is found, then
the object can be inserted by oct_subtree_insert().

   The actual insertion of the object takes place after the local
octree has been searched, and a terminal octant with the correct
bounds has been found. The object is added into the octant by the
POC_addRegion() function which appends the object to a linked list
structure that the octant's data field "list" points to. After
inserting, the number of objects associated with that octant are
checked. If the number of objects is greater than the max number of
objects allowed per octant, then the octant is refined another level
in oct_terminal_refine(). The bounds of the octant to be subdivided
are retrieved so the child bounds can be calculated. Child octants are
assigned subdivisions in the order of the space filling curve
selected. Information on all the objects are removed from the original
octant and assigned to the child octants according to their coordinate
information. Once the objects are reinserted, each child node is
checked to make sure no child octant is over the maximum number of
objects. If a child exists with excessive number of objects, then
oct_terminal_refine() is called again on that child. The upper limit
to the number calls to oct_terminal_refine() at one time is 10
refinements. Exceeding 10 refinements, probably means that the objects
being inserted are so close that they can be considered to be
overlapping. At present, there is no way to handle this problem except
to inform the user of the problem and exit the program.

5.4 Load Balancing And Partitioning The Octree
   Partitioning and load balancing occur after all objects to be
balanced have been inserted into the octree. Before partitioning can
take place, the costs associated with all the objects have to be
summed because this is what the processors use as a criterion to be
balanced. Costs are calculated in the function cost_global_compute().

   When cost_global_compute() is called, the costs of all the octants
are initialized to zero so that when the costs are summed, any extra
octants will not contribute. A recursive function
costs_subtree_compute() is called that calculates the cost of an
octant by adding the costs of all the subtrees below that octant. The
bottoming out condition for the recursion is that when a terminal
octant is reached, the costs, or weights, of the all objects
associated with that terminal octant are added together, assigned to
the "cost" field of that octant, and this total is returned to the
caller, which was the parent node. The parent node then sums all the
costs of its children, stores the cost in its data field, and again
return the cost to its parent, thus computing the costs of the local
root. All the local roots then add their costs together giving the
cost, or load, of the processor. A call to MPI_Allreduce() allows each
processor to know the global_cost, which is the total work load, and a
call to MPI_Scan() is used so that a processor can know the summed
work load of the all lowered-numbered processors, stored as
prefcost. The prefcost of processor k can be thought of as the
sum(i=0..(k-1), work load of processor i).

   Using the global_cost, prefcost, and the total number of
processors, each processor can figure out where to start
shifting their objects. The optimal cost, giving the approximate
amount of load that should be on each processor, is 
global_cost / number_of_processors. Each processor can also figure out
which partition it should start assigning work to, by dividing the
prefcost by the optimal cost. Finally the amount of load in the
current partition that a processor can assume has been assigned by
other processors (called pcost) is given by 
prefcost - (partition_number * optimal_cost).  Having this information
on each processor, load balancing can be easily done in parallel.

   To better illustrate the meaning of these variables, consider the
following example with three processors, numbered 0, 1, and 2, and a
list of 15 objects, each with a uniform weight of 1. If, before load
balancing, processor 0 has 7 objects, processor 1 has 3, and processor
2 has 5, then we would have the following computations. The load on
processor 0, 1 and 2 would be 7, 3, and 5 respectively. Adding these
loads, the global_cost would be 15, and the optimal cost would equal
(15 / 3) or 5. This means that ideally, we would like to have each
partition have a load of 5. prefcost on processor 0 would be 0 since
there are no processors before 0; processor 1 would have a prefcost of
7 and processor 2 would have 10. Processor 0 would start on partition
0 and assign 5 object to that partition, then the remaining 2 to
partition 1. Processor 1 having a prefcost of 7 would then, using
integer division, divide prefcost by optimal_cost, (7 / 5), informing
processor 1 that it should being on partition 1. It would also
calculate pcost to find that pcost = (7 - (1 * 5)) = 2. This tells
processor 1 that a load of 2 has already been assigned to partition 1
by a previous processor, namely processor 0. Processor 1 will then add
its 3 objects to partition 1 giving a total load of 5. Finally
processor 2 will do similar calculations and find that it should start
work on partition 2, and has a pcost of 0, thus allowing all of its
objects to go into partition 2.

   The actual partitioning/load balancing is done after all the costs
variables have been calculated. Each processor begins visiting each of
the local roots in order; this is done in the function
visit_all_subtrees(). The root of each subtree is visited and it is
queried for its cost. This cost is checked to see if its whole subtree
can fit in the current partition. If the cost of the subtree is less
than the space left on the partition, signifying no overflow, then the
whole subtree is marked as being in that partition. If there is an
overflow, then the subtrees below the root being currently looked at
are examined. The algorithm continues to go down the tree until it
finds a subtree that can fit in the current partition. If this
continues down to a leaf octant of the tree, and the leaf does not fit
perfectly into the partition, then a decision must be made on whether
to add it to the current partition, or start a new one. If the cost of
adding the octant's list of objects to the current partition is less
than double the amount to go, then those objects are added; otherwise,
the partition is considered full, and a new one is started. Once the
partition is filled, then pcost is reset to 0, the partition counter
is incremented, and the algorithm begins again with the subtrees that
are left.

   Once all the processors have finished marking all their subtrees,
the objects associated with each processor are now able to be
migrated. A list of all the objects to be migrated from the local
processor and a list of destination processor ids are passed into the
irregular communication functions, comm_create() and comm_do(),
developed by Steve Plimpton and Bruce Hendrickson of Sandia National
Laboratories. This handles all the message passing to each of the
appropriate processors. Once received, all new objects are placed in
an array, and passed back to the program that called the load
balancer.
